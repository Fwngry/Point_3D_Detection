>  原文地址 [zhuanlan.zhihu.com](https://zhuanlan.zhihu.com/p/103640399)

1.  Abstract：在点云深度学习中，主要包含的任务有：3D 形状分类、3D 目标检测和跟踪、3D 点云分割。

Introduction：3D 数据通常有许多种表现形式：深度图、点云、网格、体积网格 (volumetric grids)。点云表示的好处是：保持了最原始的 3D 空间中的几何信息，并且没有任何的离散化。因此在无人驾驶和机器人领域更偏向使用点云表示。

2. 3D 点云深度学习的挑战主要在于：数据集较小，3D 点云的高维度以及非结构化。

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_v2-532ae2f3198a165c8b266209b7232f4d_r.jpg)

接下来的文章组织。第二部分主要介绍 3D 形状分类；第三部分介绍 3D 目标检测和跟踪；第四部分介绍点云分割，其中包括语义分割，实例分割以及部分分割。

**2 3D 形状分类**

这类方法通常先学习到每个点的 Embedding，再使用聚合的方法， 在所有点云中提取全局的形状 Embedding。分类通常是用一些全连接层得到的。基于这种在各个点上提取特征的方法，目前的 3D 形状分类方法可以被分成**基于投影的方法**和**基于点的方法**。

基于投影的方法通常将非结构化的点云投影至中间的规则表示，接着利用 2D 或者 3D 卷积来实现形状分类。而基于点的方法直接在原始数据上进行处理，并不需要体素化或是投影。基于点的方法不引入其它的信息损失且变得越来越流行。此文章主要介绍基于点的网络。

**2.1 基于投影的方法**

这类方法将 3D 点云投影至不同的表示模态（多视角，体素表示等）来进行特征学习和形状分类。

**2.1.1 多视角表示**

这类方法首先将 3D 物体映射至多种视角，并且提取出对应的 view-wise 特征，将这些特征融合后进行准确的物体识别。如何将多视角的特征融合成一个判别的全局表示是主要的挑战。MVCNN[15] 将多视角的特征进行最大池化，从而得到全局的描述子。然而由于其中的 max pooling，最大元素仅仅来自于某个特定的视角，因此造成了一定的信息损失。MHBN[16] 通过 harmonized 双线性池化，将局部的卷积特征集成起来，产生笑容的全局描述子。[17] 利用关系网络，在视角中来得到相互关系（区域 - 区域相互关系和视角 - 视角相互关系），接着将这些视角集成起来得到判别式的 3D 物体表示。

**2.1.2 体素表示 (Volumetric Representation)**

早些的方法在 3D 点云体素表示上使用 3D 的 CNN 来提取特征。[22] 引入了体素占据网络（Volumetric Occupancy Network）VoxNet 来得到鲁棒的 3D 物体识别。[6] 提出了 3D ShapeNet，在不同的 3D 形状中学习点的分布。3D 的形状通常可以用二进制变量在体素网格上的分布来表示。由于计算量和内存随着分辨率 3 次增长，这样的方法在稠密的 3D 数据上表现较差。为了解决该问题，一些图结构（八叉树）被引入用来减小计算量和内存使用。OctNet[23] 首先将点云用网格八叉树结构分成不同层次，八叉树的结构用 a bit string representation 编码。[24] 提出了基于八叉树的 CNN 实现 3D 形状分类。[25] 提出了混合的网络 PointGrid，它将点和网格表示集成在一起。

**2.2 基于点的方法**

基于网络结构的不同，这类方法可以被分为以下几类，包括：**点光滑的 MLP（PointWise）**；**基于卷积的方法**；**基于图的方法**；**基于数据索引的方法以及其它网络**。

**2.2.1 Pointwise MLP**

这类方法使用 MLP 对各个点进行独立的建模，接着使用对称的函数来集成到全局特征。对于无序的 3D 点云数据，这类网络可以得到置换不变性。然而这样的方法并未考虑到 3D 点之间的几何关系。

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_v2-af5ae28e8a5183e1b0c335bb5dde85cd_r.jpg)

作为先驱工作，PointNet[5] 使用 MLP 学习 Pointwise 特征，接着使用最大池化层来提取全局的形状特征。最后的分类结果也使用 MLP 来得到。[26] 也论证了，得到置换不变性的关键在于将所有表示加起来并且使用非线性变化。[26] 也设计了基础的网络 DeepSets 来进行多种应用的实现，包括形状分类。

由于 [5] 中的特征是各个点独立学习到的，因此各个点之间的局部结构信息无法得到。[27]提出了一种分层次的网络 PointNet++, 从各个点之间的邻居来获取细粒度的几何特征。

因为 [5]PointNet 的简单和有效性，许多工作都基于 PointNet 开展。[28] 引入了深度自编码网络在学习点云表示。它的编码是在 PointNet 之后，并且使用 1-D 卷积、ReLU 非线性激活、BN 和最大池化来提取各个点的特征。在 [29] 中，使用每个点的绝对坐标以及相对其邻居的相对坐标对其进行表示。接着使用 Group Shuffle Attention 来获取点与点之间的关系，再使用具有置换不变性、可微分、端到端可训练的 Gumbel Subset Sampling 层来训练层级特征。[30]Mo-Net 的结构与 PointNet 类似，使用一组有限的 moments 作为网络的输入。PointWeb[31]同样基于 PointNet++，利用 Adaptive Feature Adjustment， 使用了局部邻居之间的关系来提升点的特征。[32]提出使用 Structural Relational Network 来学习不同局部结构之间的结构化关系特征。[34]首次将点云投影来获得旋转不变性的表示，并且利用基于 PointNet 的 backbone 来提取全局特征，基于图的集成方法来提取局部特征。

**2.2.2 基于卷积的网络**

与 2D 卷积相比，由于点云的不规则性，3D 点云的卷积核更难设置。根据卷积核的不同，目前的 3D 卷积网络可以被分为连续卷积网络和离散卷积网络，如下图所示。

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_v2-54aeddf393933ea09623d806d2968d3d_r.jpg)

**3D 连续卷积网络**

这类方法在连续的空间中定义卷积核，其中邻居点的权重与它和中心点的空间分布有关。3D 卷积可被认为是在给定子集上的加权。MLP 是一种简单的学习权重的方法。作为 RS-CNN[35]的核心层，RS-Conv 将某个点周围的局部子集作为其输入，使用 MLP 的方法来进行卷积，学习低维关系到高维关系的映射。（“And the convolution is implemented using an MLP by learning the mapping from low-level relations to high-level relations between points in the local subset”）。在 [36] 中，卷积核的元素是在单位球中随机选取的，接着使用基于 MLP 的连续函数来确定核元素的位置与点云的关系。在 DensePoint[37]中，卷积被定义为单层感知机（Single-Layer Perceptron）。某层的特征是将其之前所有层的特征级联起来，从而使得空间信息被充分利用。

有些方法使用已有的算法来进行卷积。在 PointConv[38] 中，卷积被定义为对重要性采样的连续 3D 卷积的蒙特卡洛估计。卷积核由加权函数（由 MLP 层学到）和密度函数（由核密度估计和 MLP 层学到）组成。为了提升内存和计算效率，3D 卷积被简化成两部分：矩阵乘法和 2D 卷积，在相同的参数设置下，内存消耗可减小 64 倍。在 MCCNN[39] 中，卷积被当做是蒙特卡洛估计的过程（依赖样本的密度函数，用 MLP 实现），使用泊松圆盘采样（Poisson disk sampling）来构建点云等级。

**3D 离散卷积网络**

这类方法在标准的网格上定义卷积核，其中的邻居点的权重是其关于中心点的补偿（offset）。[49] 将非归一化的点云变换至归一化的网格，接着在各个网格上定义卷积核。与 2D 卷积不同（在各个像素上分配权重），所提的 3D 卷积核在网格内的所有点赋予相同的权重。对于给定点，邻域内所有点（在相同网格上）的平均特征通过之前的层来计算得到。接着，所有网格的平均特征通过加权和产生当前层的输出。[50] 定义了球状的卷积核，通过将 3D 球体邻域分成多个体素 bins，并且将各个 bin 通过学到的加权矩阵联系到一起。球状卷积核的输出由其邻域点的加权均值 通过非线性激活层得到。

**2.2.3 基于图的网络**

基于图的网络将点云中的各个点当做是图中的顶点，然后产生有向边。接着在空间中或谱域中学习特征。典型的基于图的网络如下图所示。

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_v2-5190b4aed1eef1345402cb8059f5c97d_r.jpg)

**空间域中的基于图的方法**

这类方法在空间域中定义卷积和池化操作。卷积通过在空间邻域内的 MLP 实现，池化操作通过集成信息产生新的较粗的图。各个顶点的特征由坐标、激光强度、颜色来确定，各个边的特征由两个连接点的几何属性确定。

作为先驱工作，[58] 将各个点视为图的顶点，利用有向边将顶点与其邻域内的点相连，接着使用 Edge-Condition Convolution（使用生成 filter 的网络得到，MLP 等）。最大池化用来集成邻域信息，图的粗化使用 VoxelGrid[59] 算法得到。首先通过卷积和池化的相互交错，再跟着为全局平均池化和全连接层来产生分类 score。

**谱域中的基于图的方法**

这类方法将卷积定义为谱的滤波，是通过在图上的拉普拉斯矩阵的特征向量上的乘法来实现的 [70][71]。与上述方法不同的是，RGCNN[72] 将点云中的点与其余所有的点相连得到图，接着在各层当中更新图的拉普拉斯矩阵。为了使相邻顶点的特征更加相似，图信号光滑的先验被加入到了损失函数中。

**2.2.4 基于索引数据的网络**

这类网络是基于不同的数据索引结构（八叉树、KD 树）构建的。在这类方法中，点的特征是从叶节点到根节点中分级学习的道德。[50] 提出了八叉树导向的 CNN，利用了球状卷积核。网络中的每一层对应八叉树的一层，球状的卷积核在各层中均使用。当前神经元的值是：在之前层中相关的子节点的均值。而 Kd-Net[78] 由多重的 K-d 树构成，这些 K-d 树在每次迭代时有着不同的分裂方向。根据从下到上的方法，非叶节点的表示是由其子节点通过 MLP 计算得到的。根节点的特征（描述整个点云）最终被送入至全连接层来得到分类输出。值得注意的是，Kd-Net 在各级之间共享参数。3DContextNet[79] 使用标准的平衡 K-d 树进行特征提取和集成。

**2.2.5 其它网络**

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_07_15_v2-b232d5a1b6548bdf4b3aa2957af8f9a6_r.jpg)

**3 3D 物体检测与跟踪**

**3.1 3D 物体检测**

与普通 2D 中的目标检测方法类似，3D 中的目标检测方法也可分为两类：基于候选区域的方法和直接映射的方法。

**3.1.1 基于候选区域的方法**

这些方法首先产生一些可能包含物体的区域（Proposals），接着对各个区域提取特征，来决定各个候选区域的物体类别。

根据不同的产生候选区域的方法，这些方法可进一步分为三类：基于多视角的方法；基于分割的方法以及基于锥体的方法。

**3.1.1.1 多视角的方法**

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_v2-21c5b2c69267095872067fb1abb4819c_r.jpg)

这类方法从不同的视角图像（雷达前景图，鸟瞰图，图像等）中融合各个候选框的特征，来产生 3D 旋转盒。在 [4] 中，Chen 等人从鸟瞰图中产生一组准确的 3D 候选框，并且将其投影到其它视角中（雷达前景图，RGB 图像），接着将各个区域的特征组合到一起，来预测有方向的 3D bounding boxes。尽管这种方法在 0.25IOU， 300 个候选框设置时达到了 99.1% 的 recall，但是速度非常慢。后续的基于多视角的 3D 物体检测方法主要从以下两个方面来提升。

（1）提出了很多方法来有效的融合不同模态之间的信息。为了针对小物体产生有较高 recall 的候选框，[97] 提出了多模态的基于融合融合的区域生成网络。首先从鸟瞰图和普通图像视角提取相同大小的特征，然后在各个元素位置使用了平均池化来融合特征。[98] 利用了连续的卷积来进行图像与 3D 雷达前景图的特征融合。具体而言，他们对 BEV（鸟瞰视角）空间中的每个点提取最近的对应点的图像特征，接着通过将图像特征投影至 BEV 空间的方法，使用双线性插值得到稠密的 BEV 的特征图。**实验结果证明稠密的 BEV 特征图比起离散的图像特征图和 LiDAR(雷达激光) 特征图更加适合 3D 物体检测。**[99]提出了多任务，多感知器的 3D 物体检测网络来进行端到端的训练。具体而言，利用多种任务（2D 物体检测，背景估计，深度补偿），帮助网络学习到更好的特征表示。学习到的跨模态的表示，可进一步用来产生更准确的物体检测结果。实验证明这类方法在 2D,3D,BEV 识别任务上有着非常好的提升，在 TOR4D 基准 [100, 101] 上超越了之前的 SOTA。

（2）其它的一些方法致力于提取更鲁棒的表示。[102]通过引入空间 Channel 注意力机制模块（Spatial Channel Attention Module）, 探索了多尺度的环境信息，其捕获了全局的以及多尺度的场景环境，加强了有用的特征。同样的，通过将不同尺度的低层次特征融合的方法，他们提出了 Extension Spatial Unsample 模块来得到有着更丰富空间信息的高层次特征，接着来产生更可靠的 3D 物体候选框。尽管达到了更好的检测效果，但上述所提的多视角方法都需要较长的运行时间，因为他们在各个候选框都进行了特征的池化。因此，[103]使用了提前的 ROI 池化卷积（pre-ROI pooling convolution）来提高 [4] 的效率。具体而言，他们将大部分的卷及操作移动到 ROI pooling 模块之前。因此，对于所有的物体候选框，ROI 卷积只使用一次。实验结果显示这类方法可达到 11.1fps, 速度达到了 MV3D[4]的 5 倍。

**3.1.1.2 基于分割的方法**

这类方法首先利用现有的语义分割技术来移除多数的背景点，接着在前景点上，产生大量的高质量的候选框来节约计算。与多视角的方法 [4],[97],[103] 相比，这类方法达到了更好的物体 recall，并且更适合一些复杂的场景。

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_v2-5a992f99c6a6b97842acb773f5db3af1_r.jpg)

[104]中，Yang et al 使用了 2D 的分割网络来预测前景的像素并将其投影至点云中，以此来剔除掉多数的背景点。接着在这些前景点中生成候选框，并且设计了一种新的标准称之为 PointsIoU 来减少候选框的冗余性和模棱两可之处。跟着 [104] 的脚步，[105]提出了 PointRCNN 的框架。具体而言，他们直接对 3D 点云进行分割，得到前景点，并且将语义特征和局部空间特征融合从而得到高质量的 3D boxes。[106] following [105]中的 RPN，提出了一种利用图卷积网络来进行 3D 物体检测。具体而言，利用图卷积，在此处引入了两个模块来修复物体的候选框。第一个模块 R-GCN 利用一个候选框中的所有点，得到每个候选框的特征集成。第二个模块 C-GCN 将所有候选框中的每一帧信息融合起来，利用环境来回归准确的物体 boxes。[107]将点云投影至基于图像的分割网络的输出，将其附加至语义的预测值。[109]得到了显著的性能提升，通过将涂色的点送入至一些检测器中 [105, 108]。[110] 将每个点与 spherical anchor 相关联，每个点的语义值用来移除多余的 anchors。这样的方法得到了更好的 recall 以及有着更小的计算消耗。与此同时，文中提出了 PointsPool 层，对候选框中的内部点学习相容的特征，并且引入了并行的 IOU 来提高位置的准确度的检测性能。实验结果正式这样的方法在 KITTI 数据集 [10] 上较难的集合（car class）的性能比 [99, 105, 111] 的性能优越很多，并达到了 12.5fps。

**3.1.1.3 基于椎体的方法**

这类方法首先利用现有的 2D 物体检测子，产生 2D 的候选矩形框，接着对每个 2D 的候选框提取 3D 的锥体候选框，如下图所示。尽管这类方法可以有效地给出 3D 物体的坐标，但 step-by-step 的 pipeline 使得性能受限（受限于 2D 图像的检测子）。

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_v2-94199e151e5bcfafca8dd03348ceb8a4_r.jpg)

F-PointNets[112]为此类方向的先驱工作。它在每个 2D 区域上产生锥形的候选框，并且应用 PointNet[5](或 PointNet++[27])来学习各个 3D 锥体的点云特征，从而进行 3D box 的估计。在随后的工作中，[113]提出了 Point-SENet 模块，来预测一系列的缩放因子，从而被用来突出有用特征和抑制无用特征。同时他们也将 PointSIFT[114]模块集成至网络中，来获取点云的方向信息，其可以得到对形状尺度的强鲁棒性。该方法在 [10], [115] 的数据集上，与 F-PointNets[112]相比得到了显著的提高。

方法 [116] 利用了 2D 图像区域和对应的锥体点来回归 3D boxes。为了融合图像特征和点云的全局特征，他们提出了全局的融合网络来直接回归 box 的角坐标。他们也提出了稠密的网络网络来预测各个点对于各个角的补偿（offsets）。[117]第一次从 2D 图像中估计 2D 的 bounding boxes 和 3D 物体姿态，从而提取物体候选框。这类 3D 候选框被送入至 box 回归网络来预测准确的 3D 物体 boxes。[111]对于各个 2D 区域，在锥体轴上产生一系列的锥体，并使用 PointNet 来对各个锥体提取特征。锥体层次的特征用来产生 2D 特征图，再被送入至 FCN 来估计 3D box。该方法在基于 2D 图像的方法中达到了 state-of-the-art 的性能，并且在 KITTI 积分榜上排在很靠前的位置。[118]首先在鸟瞰图上得到初步的检测结果，接着基于鸟瞰图的预测结果，提取小部分点的子集，再应用局部的微调网络来学习局部特征，预测高精度的 3D bounding boxes。

**3.1.1.4 其它方法**

受轴对齐的物镜在图像中检测物体的成功启发，Zhou等人[146]将两个三维旋转边界盒的物镜整合到几个最先进的检测器中[133], [137], [158]，实现了性能的持续改进。

Chen等人[147]提出了一个两阶段的网络结构，以使用点云和体素表示。首先，点云被体素化并被送入一个三维骨干网络以产生初始检测结果。其次，初始预测的内部点特征被进一步利用来进行箱体细化。虽然这种设计在概念上很简单，但它实现了与[133]相当的性能，同时保持了16.7fps的速度。

Shi等人[148]提出了PointVoxel-RCNN（PV-RCNN），利用3D卷积网络和基于PointNet的集合抽象来学习点云特征。具体来说，输入的点云首先被体素化，然后被送入一个三维稀疏卷积网络以产生高质量的建议。然后通过体素集抽象模块将学到的体素特征编码为一小部分关键点。此外，他们还提出了一个关键点到网格的ROI抽象模块，以捕捉丰富的背景信息，用于盒子的细化。实验结果表明，该方法的性能明显优于以前的方法，在KITTI三维检测基准的汽车类中排名第一1。

受基于Hough投票的二维物体检测器的启发，Qi等人[124]提出了VoteNet，直接对点云中物体的虚拟中心点进行投票，并通过聚合投票特征生成一组高质量的三维物体建议。VoteNet明显优于之前只使用几何信息的方法，并在两个大型室内基准测试（即ScanNet[11]和SUN RGB-D[25]）中取得了最先进的性能。然而，对于部分遮挡的物体，虚拟中心点的预测是不稳定的。此外，Feng等人[149]增加了一个方向向量的辅助分支，以提高虚拟中心点和三维候选框的预测精度。此外，还建立了一个提案之间的三维物体-物体关系图，以强调准确的物体检测的有用特征。

Qi等人[150]提出了一个ImVoteNet检测器，将二维物体检测线索（如几何和语义/纹理线索）融合到三维投票管道中。
Shi等人[151]受三维物体的地面真实盒提供物体内部零件的准确位置这一观察结果的启发，提出了Part-A2网，它由一个零件感知阶段和一个零件聚合阶段组成。

零件感知阶段应用类似UNet[165]的网络工作，通过稀疏卷积和稀疏去卷积来学习点状特征，用于预测和粗略生成物体内部的零件位置。零件聚合阶段采用RoI感知的集合，以聚合预测的零件位置，进行箱体细化。

**3.1.2 Single Shot Methods**

这类方法使用单阶段的网络，直接预测类别概率和回归物体的 3D bounding boxes。这类方法不需要产生区域候选框和后处理。结果是，这类方法有着很快的速度，很适合实时的应用。根据输入数据的形式，single shot 方法可分为两类：基于鸟瞰图的方法和基于点云的方法。

**3.1.2.1 基于鸟瞰图的方法**

这类方法将 BEV 表示作为输入。[100]将场景的点云离散化，使用 FCN 来预测位置和物体的航向角。该方法超越了大多数的 single shot 方法 ([125], [126], [127]) 并且达到了 28.6fps。之后，[128]利用 HP map 提供的几何和语义先验信息，提高了 [100] 的鲁棒性和检测性能。

**3.1.2.2 基于点云的方法**

这类方法将点云转换至一般的表示（例如 2D map），接着使用 CNN 来预测各个类别和 3D boxes。[125]提出了使用 FCN 进行 3D 物体检测。他们将点云转换至 2D point map，使用 2D FCN 来预测 bounding boxes 和物体的置信度。之后，[126]将点云离散化至 4D 的张量，其维度分别为：长度，宽度，高度和 channel，接着将 2D FCN 的方法延伸至 3D 来进行 3D 的物体检测。与 [125] 相比，基于 FCN 的 3D 方法达到了大于 20% 准确率的收益，但是由于 3D 卷积核数据的稀疏性，消耗了更多的计算资源。为了借体素稀疏性的问题，[127]利用了 feature-centric voting scheme，对每个非空的体素来产生一系列的 votes，最后通过将 votes 相加的方式得到卷积的结果。它的计算复杂度与被占据的体素数量成正比。[130]通过堆叠多个稀疏 3D CNN，构建了 3D 的 backbone 网络。这样的设计节约了内存并且加速了计算。这个 3Dbackbone 网络提取了丰富的物体检测的 3D 特征，并且并未引入计算量的负担。

[108]提出了基于体素的端到端的可训练框架 VoxelNet。他们将点云分入等空间尺寸的体素，将每个体素的特征编码成 4D 的张量。RPN 网络用来产生检测结果。尽管该方法效果很好，但由于体素的稀疏性和 3D 卷积操作，该方法运行速度很慢。之后，[120]使用了稀疏的卷及网络 [134] 来提高 [108] 的推断效率。同时他们提出了 sine-error angle loss 来解决方向 0 和 Pi 的歧义。[131]通过将图像和点云特征在早期融合的方式，扩展了 VoxelNet 的工作。具体而言，他们将 [108] 产生的非空体素投影至图像，使用预训练的网络对各个投影的体素提取图像特征。这些图像特征与体素特征相级联，来预测准确的 3D boxes。这类方法利用了多模态的信息，来减少 false postivies and negatives。[109]提出了 3D 物体检测子称为 PointPillars。该方法利用了 PointNet 来学习点云的特征，将这些学到的特征编码成 pesudo images。利用 2D 的物体检测 pipeline 来预测 3D bounding boxes。PointPillars 在 Average Precision 的指标上，超越了大多数的融合方法（MV3D[4], RoarNet[117], AVOD[97]）。并且，PointPillars 在 3D 和 BEV KITTI benchmarks 上达到了 62fps。

**3.1.2.3 其余方法**

[132] 提出了一种有效的 3D 目标检测子称之为 LaserNet。鬼方法在各个点上预测 bounding boxes 的概率分布，接着讲各个点的分布组合起来产生最后的 3D 物体 boxes。接着，点云的 dense range view representation 作为输入，使用快速的 mean-shift 算法来减少噪声。LaserNet 达到了 0-50 米范围的 SOTA 性能，并且运行时间很少。[133] 接着讲 LaserNet 扩展至稠密的纹理信息。该方法在 50-70 米范围内得到了非常好的性能。

**3.2 3D 物体跟踪**

给定一个物体在第一帧时的位置，目标跟踪的任务是估计它在之后帧的状态。由于 3D 物体跟踪可以使用点云中丰富的几何信息，人们期待用它来克服在 2D 图像上追踪任务的困难，包括遮挡，光照以及尺度的变化。

由于孪生网络 [137] 在目标跟踪上的成果, [138]提出了有着形状补偿正则化的 3D 孪生网络。具体而言，首先用 Kalman 滤波器产生 candidates，接着利用形状正则化，将模型与 candidates 编码成相容的表示，使用余弦相似度来寻找被追踪物体下一帧的位置。该方法可当做目标跟踪中的一种可选方案，并且超越了大多数的 2D 目标跟踪方法，包括 Staple-CA[139]和 SiamFC[137]. 为了更有效地寻找目标物体，[140]利用了 2D 孪生网络，在 BEV 上产生大量的粗略的物体候选框。接着利用 3D 孪生网络中的余弦相似度来修正候选框。该方法在 precision 和 success rate 上显著地超越了 [138]。[141] 提出了 3D 物体检测和跟踪的框架，应用于有语义信息的点云上。首先通过融合 2D 视觉语义信息，产生体素化的带语义的点云，急着利用短时的信息来提高多物体跟踪的准确性和鲁棒性。此外，他们引入了有效、简化的度量（Scale-Rotation-Translation scores, SRFs）来加速训练和度量。他们提出的 Complexer-YOLO 达到了有前景的跟踪性能，并且可以达到实时。

**3.3 3D 场景流估计**

与 2D 视觉中的光流场估计类似，许多方法开始在一系列的点云中学习一些有用知识。

[142] 提出了 FlowNet3D，在一系列连续点云中直接学习场景流。FlowNet3D 通过 flow embedding layer， 学习 point-level 的特征和运动特征。然而 FlowNet3D 存在两个问题。第一，一些预测的运动向量与真实值差别非常大；第二，很难将 FlowNet 应用至非静态的场景，尤其是有着可形变物体的场景。为了解决该问题，[143] 引入了余弦距离的损失函数来最小化预测值与真实值之间的夹角。同时，他们提出了 point-to-plane 的距离损失函数，来提高刚性的和动态的场景的准确率。实验结果显示这两种损失函数将 FlowNet3D 的准确率从 57.85% 提升至 63.43%，并且加速和稳定了训练过程。[144] 提出了 HPLFlowNet，从大规模的点云中直接估计场景流。文中提出了一些 bilateral convolutional layers 来存储结构信息，同时降低计算消耗。

为了有效地处理序列点云，[145] 提出了 PointRNN, PointGRU 和 PointLSTM，以及一个序列到序列的模型来追踪移动点。PointRNN, PointGRU 和 PointLSTM 能够捕捉空间 - 时间信息，并且建模动态的点云。类似地，[146] 提出了 MeteorNet 来直接从动态点云中学习表示。该方法试图从时间和空间上的邻近点学习总体特征。[147] 提出额两个自监督的损失函数，在大量无标签的数据集上训练网络。他们的主要思想是：一种鲁棒的场景流估计方法应该在向前预测和向后预测时均有效。由于场景流标注的不可行性，预测得到的转换后的点的最近点，被当做是假想的真实值。然而，真正的真实值可能与它不同。为了避免这个问题，他们在相反的方向计算场景流，并且提出了 cycle consistency loss。实验结果显示这种自监督的方法超过了现有自监督学习方法中的 SOTA 性能。

**3.4 总结**

KITTI 基准是自动驾驶领域中最有影响力的，并且在学术和工业领域有着广泛的应用。表 2 和表 3 展示了不同方法在 KITTI test 3D and BEV benchmark 上的结果。

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_07_15_v2-9bf3b4bb855322425e1097a6bff8c3ad_r.jpg)![](https://pic1.zhimg.com/v2-51ca86700a6ef0515d87d0fb961ebc88_r.jpg)

可以观察到：

a. 基于区域生成的方法是最常见的方法，在 KITTI test 3D， BEV 上的性能均超出了 single shot methods。

b. 现有的 3D 目标检测子有两个限制。第一，长范围的检测能力较弱。第二，如何全面的利用纹理图像仍然是个公开的问题。

c. 多任务学习是在 3D 目标检测中未来的方向。例如，[99] 通过合并多种任务，学习跨模态的表示来得到 SOTA 的检测效果。

d. 3D 物体跟踪和场景流估计是较新的研究方向。

**4. 3D 点云分割**

3D 点云分割要求了解全局的几何结构以及各个点的细粒度的细节。根据分割的细粒度，3D 点云分割方法可分为以下三类：语义分割（场景级）、实例分割（物体级）和 part segmentation（part level）。

**4.1 3D 语义分割**

给定一个点云，语义分割的目标是，根据语义信息，将各个点分成一定的子集。与 3D 形状分类的分类类似，语义分割可分为两种方法：基于投影的方法和基于点的方法。

**4.1.1 基于投影的网络**

Intermediate regular representations 可被分成以下几种：多视角表示 [148], [149]、球状表示[150], [151], [152]、体素表示[153], [154], [155]、超多面体晶格表示[156], [157] 以及混合表示[158], [159]。具体可见下图。

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_v2-66bb7d8c7837d7710d43a886cca19a15_r.jpg)

**4.1.1.1 多视角表示**

[148] 首先将 3D 点云从多个虚拟的相机视角投影至 2D 平面上，接着，使用多流的 FCN 在人造的图像上预测各个像素的 scores。最终各个点的语义标签为不同视角上 scores 的融合。相似地，[149] 首先利用多组相机的位置，得到点云的一些 RGB 和深度图。接着使用 2D 的分割网络，在这些图片上得到各个像素的 label，这些从 RGB 和深度图上得到的 scores 用来融合（使用 redisual correction[160]）。基于点云是从局部欧式曲面上采样得到的假设， [161] 引入了 tangent convolutions 进行稠密的点云分割。该方法首先将各个点周围的局部曲面投影至虚拟的切平面。Tangent convolutions 在曲面上直接进行。总的来说，多视角分割方法的性能对视角的选择和遮挡非常敏感。同时，这类方法并未能完全利用潜在的几何和结构信息，因为投影操作不可避免地引入了信息损失。

**4.1.1.2 球状表示**

为了得到更快更准确的 3D 点云分割，[150] 提出了基于 SqueezeNet 和条件随机场的端到端的网络。为了进一步提升分割准确率，引入了 SqueezeSegV2[151] 利用无监督的 domain adaptation 解决 domain shift。[152] 提出了 RangeNet++，针对 LiDAR 点云进行实时的语义分割。2D 深度图的语义标签首先转移至 3D 点云上，接着使用基于 KNN 的后处理步骤来减轻离散化误差的问题。与单一的视角映射相比，球映射保持了更多的信息，并且更适合 LiDAR 点云。然而，这样的中间表示不可避免地引入了一些问题，比如离散化误差和遮挡问题。

**4.1.1.3 体素表示**

[163] 首先将点云分成一系列占有的体素。接着将这些过渡数据送入至 fully-3D CNN 中进行体素级别的分割。最后，一格体素中的所有点的语义信息与该体素的 label 相同。该方法的性能极其受限于体素的细粒度和边界的伪影。之后，[164] 提出 SEGCloud 来得到更细粒度和 global consistent 的语义分割。该方法引入了确定性的三线性插值，将由 3D-FCNN 产生的粗糙的网格预测映射回点云中，接着使用 Fully Connected CRF，确保推测出的点云有着空间上的一致性。[153] 引入了一种基于核的变分自编码器结构地局部几何结构进行编码。这里摒弃了 binary occupancy representations， 使用 RBF 得到连续的表示，并且捕获到每个体素中点的分布。再使用 VAE 将各个体素中的点映射至隐空间，最后使用 CNN 得到鲁棒的特征表示。

良好的尺度扩展性质是体素表示中的优点之一。具体而言，基于体素的网络对于有着不同空间尺寸的点云，其训练和测试是不受限制的。在 Fully-Convolutional Point Network（FCPN）中，不同级别的几何相关性从点云中提取出来，再使用 3D 卷积核加权的均值池化来提取特征。该方法可处理大规模的点云，并且在推断时有着良好的尺度扩展性质。[166] 提出了 ScanComplete 来实现 3D 补全，以及对各个体素进行语义预测。该方法利用了全卷积网络的尺度扩展性，在训练和测试阶段应对不同的输入数据大小。使用从粗到细的策略来提高预测结果的分辨率。

很自然地，体素表示是稀疏的，其中非零元素的数量仅仅占很小一部分。因此，在空间上稀疏的数据使用稠密的卷积网络是比较无效的。为此，[155] 提出了子流形的稀疏卷积网络。该方法显著地减小了内存拟合计算消耗，通过限制卷积的输出只能与被占据的体素有关。同时，该稀疏卷积也可以通知提取出的特征的稀疏性。该子流形稀疏卷积很适合处理高维度且空间较稀疏的数据。更进一步，[167] 提出的 “MinkowskiNet”，即 4D 时间 - 空间卷积网络用以 3D 视频感知。广义上的稀疏卷积来处理高维数据。

综上所述，体素表示很自然地保留了 3D 点云的邻域结构。其规范的数据形式也是的标准的 3D 卷积可师姐应用。这些因素在这一领域有着不错的性能提升。然而，体素化的过程内在地引入了离散化的伪影和信息损失。通常，高分辨率会导致较高的内存和计算消耗，低分辨率引入了信息的损失。在实际中如何选择合适的网格分辨率是 non-trivial 的。

**4.1.1.4 超多面体晶格表示**

[156] 提出了基于双边卷积层（Bilateral convolution layers）的稀疏晶格网络（Sparse Lattice Networks）。该方法首先将原始点云插入至超多面体稀疏晶格，再使用 BCL 对占据的部分进行卷积。得到的输出再重新映射回原始点云。更进一步，[157] 提出了 LatticeNet 来实现有效的处理大规模点云。

**4.1.1.5 混合表示**

为了进一步利用所有可用信息，许多方法试图学习多模态特征。[158] 提出了 joint 3D-mult-view 网络，来组合 RGB 和几何特征。一个 3D CNN 流和一些 2D CNN 流用来提取特征，另一个可微分的 back-projection layer 用来合并 3D 和 2D 特征。更进一步，[168] 提出了 unified point-based network 来学习 2D 纹理信息，3D 结构和全局特征。该方法直接应用基于点的网络来提取局部几何特征和环境信息。[159] 提出了 Multiview PointNet（MVPNet）来集成 2D 多视角特征和空间几何特征。

**4.1.2 基于点的网络**

基于点的网络直接在点云上进行操作。然而，点云通常是无序且无结构的，使得直接应用标准的 CNN 不现实。为此，先驱的工作 PointNet[5] 用来对每个点进行特征学习，使用的是标准的 MLP 和全局特征。基于 PointNet，一系列基于点的网络被提出。总体而言，这类方法可悲简单的分为以下几类：基于各个点的 MLP 方法，基于点卷积的方法，基于 RNN 的方法和基于图的方法。

**4.1.2.1 Pointwise MLP Methods**

这类方法通常利用共享的 MLP 作为网络中的基本单元。然而，由共享 MLP 提取出的各个点上的特征，并不能获取到点云中的局部几何关系，以及点与点之间的关系 [5]。为了获取各个点周围更广泛的信息，以及学习到更丰富的局部结构，有很多方法被提出，包括基于邻近点特征池化的方法，基于注意力机制的集成(attention-based aggregation) 以及局部 - 全局的特征级联。

**Neighboring feature pooling：** 为了获取局部的几何形式，这类方法通过将局部邻域点集成的方式，对各个点学习特征。具体而言，PointNet++[27] 将点分层次，逐步地分成一些组，如下图所示。多尺度的 grouping 和多分辨率的 grouping 来克服点云多样性造成的问题。之后，[114] 提出了 PointSIFT 模块来实现方向的编码和 scale awareness。该模块通过使用 3 阶段的有向的卷积操作，将 8 个空间方向的信息堆叠并且编码，将多尺度的特征提取并级联来实现对不同尺度的适应性。与 PointNet++ 中使用 GROUPING 的方法不同，[169] 利用 K-Means 聚类和 KNN 的方法在世界空间和特征空间定义两种邻域。基于这样的假设：来自于同一类的点在特征空间中应当接近，该论文提出了 pairwise distance loss and a centroid loss 来对特征学习进行正则。为了建模点与点之间的相互关系，[31] 提出了 PointWeb 来寻找局部区域内所有点对之间的关系。[170] 提出了置换不变性的卷积称之为 Shellconv。[95] 提出了有效、轻量的网络称为 RandLA-Net 实现大规模的点云处理。该方法利用随机样本采样，在内存和计算方面提升很多。提出的局部特征集成用来获取和保持几何特征。

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_v2-fddd48034e333a5205012613873e51e2_r.jpg)

**Attention-based aggregation：**为了进一步提升分割的准确率，[90]针对点云分割，提出了基于注意力的机制。[29]提出了组随机注意力机制 (group shuffle attention) 来建模点之间的关系，并且提出了具有置换不变性、task-agnostic 以及可微分的 Gumbel Subset Sampling(GSS)来替代被广泛应用的 Furthest Point Sampling(FPS)方法。该方法对离群点不敏感，并且可以选择具有代表性的点的子集。为了更好地获取点云的空间分布，[171]提出了 Local Spatial Aware(LSA)层来学习空间感知权重。与 CRF 类似，[172]提出了 Attention-based Score Refinement(ASR)模块对分割的结果进行后处理。初始分割结果通过 pooling 的方式进行修正。该模块很容易被集成至其他的深度网络中来提升分割效果。

**Local-global concatenation：**[85] 提出了置换不变性的 PS2-Net，将点云的局部结构和全局信息合并。Edgeconv[60] 与 NetVLAD[173] 重复的级联起来，来获取局部信息和场景级别的全局特征。

**4.1.2.2 Point Convolution Methods**

这类方法通常试图提出在点云上进行更有效的卷积操作。[49]提出了在各个点上的卷积操作，其中邻域点被分入至 kernel cell，卷积时卷积核有权重。[174]提出了称之为 PCCN 的网络，该网络基于参数化的连续卷积层。该层的核参数通过 MLP 和张成的连续向量空间所参数化。[42]提出了 Kernel Point Fully Convolutional Network(KP-FCNN)，基于 Kernel Point Convolution(KPConv). 具体而言，KPConv 的卷积权重由欧式空间的距离决定，卷积核的点数也并不固定。卷积核点的位置由一个最优化问题确定。在 [175] 中，作者提供了丰富的消融实验（ablation experiments）和可视化结果展示了集成方法中，感受野的重要性。同时他们提出了 Dilated Point Convolution(DPC)操作，来集成邻近点的特征，进而取代 KNN 的方法。该方法在提升感受野上非常有效，并且可以容易地集成至 aggregation-based networks。

**4.1.2.3 RNN-based Methods**

为了获取点云中的内在环境特征，RNN 也比用来进行点云的语义分割。基于 PointNet[5]， [180]首先将一大块点云转换成多尺度的块和网格块来获取输入级别的环境。接着，使用 PointNet 对各个块提取特征并送入 Consolidation Units 或 Recurrent Consolidation Units 来获取输出级别的环境信息。实验结果显示，这样处理空间环境信息的方法在提高分割性能时是很重要的。[179]提出了一种轻量的模块，利用了 slice pooling layer 将无序的点云特征转换成有序的特征向量。[181]提出了 Pointwise Pyramid Pooling (3P)模块来获取从粗到细的局部特征，并利用双向的 RNN 来实现端到端学习。然而这类方法损失了丰富的几何特征和密度分布 [189]。[189] 提出了 Dynamic Aggregation Network(DAR-Net)来同时考虑全局场景复杂度和局部几何特征。[190]提出了 3DCNN-DQN-RNN。该网络首先使用 3DCNN 学习空间分布和颜色特征，使用 DQN 进一步定位类别物体。最后级联的特征向量送入 RNN 中获取最后的分割结果。

**4.1.2.4 Graph-based Methods**

为了获取 3D 点云中潜在的形状和几何结构，一些方法使用了图神经网络。[182]将点云看做是一些相连的简单形状和 Superpoint 的集合，并且使用有向图来获取结构和环境信息。接着讲大规模的点云分割问题分成三个子问题，即，geometrically homogeneous partition, superpoint embedding and contextual segmentation. 为了进一步提升，[183]提出了有监督的框架 to oversegment a point cloud into pure superpoints. 为了更好地获取高维空间中的局部几何关系，[191]提出了基于 Graph Embedding Module(GEM)和 Pyramid Attention Network(PAN)的网络 PyramNet。GEM 模块将点云表述为有向无环图，并且在构建相似度矩阵时，利用协方差矩阵代替欧式距离。在 PAN 模块中，使用 4 个不同尺寸的卷积核来提取特征。在 [184] 中，提出的 Graph Attention Convolution 用来选择性地提取特征。

**4.2 Instance Segmentation**

与语义分割相比，实例分割更具有挑战性因为它需要更准确和更小的细粒度，具体而言，他不仅需要将有着不同语义的点分辨出来，还需要将有着相同语义的实例分出来。总体而言，目前的方法可分为两个方向：基于候选框的方法以及不需要候选框的方法。一些里程碑式的方法具体见下图。

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_v2-78929e54a8fc6476464ba71b8f0847ef_r.jpg)

**4.2.1 Proposal-based Methods**

这类方法将实例分割问题分成两个子任务：3D 物体检测和实例 mask 的预测。[192] 提出了 3D fully-convolutional Semantic Instance Segmentation (3D-SIS) network，来实现在 RGB-D 数据上的语义实例分割。该网络从颜色和几何中学习特征。与 3D 目标检测类似，3D Region Proposal Network(3D-RPN) 和 3D ROI layer 用来预测 bounding box 的位置，物体类别和实例的 mask。根据合成分析策略，[193] 提出了 Generative Shape Proposal Network(GSPN) 来产生 3D 候选框。这些候选框再通过 R-PointNet 修正。最终的标签通过预测各个点的二进制 mask 来得到。与直接从点云中回归 3D bounding boxes 不同，该方法移除了许多无用的候选框。[194] 通过将 2D panoptic 分割延伸至 3D 映射，提出了在线的体素化 3D 行社系统来实现大规模的 3D 重建，给语义标签以及实例分割。该方法首先利用 2D 语义和实例分割获得像素级别的 panoptic 标签，接着将这些标签集成至体素 map 中。使用全连接的 CRF 来实现准确的分割。该语义映射系统可得到高质量的语义映射和具有判别性的目标检测。[195] 提出了单阶段的，不需要 anchor 的可训练网络称为 3D-BoNet，来实现点云上的实例分割。该方法对所有可能的实例直接回归大致的 3D bounding boxes，接着利用点级别的二分类器来获取实例标签。特别地，该 bounding boxe 的任务是被当做是最优分配问题。同时，使用了 multi-criteria 损失函数来正则化生成的 bounding boxes。该方法不需要任何的后处理操作，并且有很高的计算效率。[196] 提出了针对大规模户外 LiDAR 点云进行实例分割的网络。该方法使用 self-attention blocks，在点云的鸟瞰图上学习特征表示，最后获取的实例分割标签基于预测的水平中心和高度限制。

总的来说，基于候选框的方法较为直观，并且实例分割的结果通常较好。然而该方法需要多阶段的训练并且需要对多余候选框进行裁剪。因此通常都需要更多的时间和计算资源。

**4.2.2 Proposal-free Methods**

不需要候选框的方法 [197-202] 并没有目标检测的模块。作为替代的是，他们通常将实例分割认为是语义分割后的聚类步骤。具体而言，需要现有的方法都基于这样的假设：属于同一实例的点应当有着相似的特征。因此这类方法通常聚焦于判别式的特征学习和点云聚类。

[197] 作为先驱性的工作，首次提出了 Similarity Group Proposal Network(SGPN)。该方法首先对每个点学习特征和语义 map，接着引入相似度矩阵来表示各对点之间的相似度。为了学习到更多的判别式特征，使用了 double-hinge loss 来互相适应相似度矩阵和语义分割的结果。最后使用启发式的 NMS 方法将相似的点归并进一个实例中。由于相似度矩阵的构建需要大量的内存消耗，该方法应用的规模收到了限制。类似地，[201] 首先利用子流形稀疏卷积来预测各个体素的语义 scores，急着引入聚类算法将点聚集至实例中。更进一步，[202] 提出了 structure-aware loss 来学习判别式的 embeddings。该损失函数同时考虑了特征的相似度和点之间的几何关系。最后使用基于注意力机制的 graph CNN 来自适应地对特征进行修正。

由于一个点的语义分类和实例标签通常互相决定，许多方法将该两个任务合并成一个任务。[198] 通过引入端到端可学习的 Associatively Segmenting Instances and Semantics （ASIS）模块，将两个任务集成到一起。实验结果显示语义特征和实例特征可相互支撑，达到了一定的性能提升。[199] 首先引入了 Multi-Task Point-Wise Network(MT-PNet), 给各个点分配标签信息，并且对特征空间的 embedding 进行正则。接着将预测的语义标签和 embeddings 融合至 Multi-Value Conditional Random Field(MV-CRF) 进行联合优化。最后，mean-field variational inference 用来得到语义标签和实例标签。[204] 提出了 Dynamic Region Growing (DRG) method，自动地将点云分成一系列的块，接着使用无监督的 K-means++ 算法进行聚类。接着在环境信息的指导下进行大规模的 patch segmentation。最后，这类有标签的 patches 融合进物体级别，得到最后的语义和实例标签。

为了实现在整个 3D 场景上的实例分割，[200]提出了混合的 2D-3D 网络，该网络学习全局一致性的实例特征和局部的几何特征。学习到的特征被组合起来实现语义和实例分割。在将点聚集成实例时，并非使用 GroupMerging 算法 [197]，而是更灵活的 Meanshift[205]。[206] 同时学习了每个实例的独特的特征表示，以及对于物体中心的方向信息。使用 feature embedding loss and directional loss 在隐空间中学习特征。Mean-shift 聚类和 NMS 用来将体素积聚成实例。该方法在 ScanNet[8]基准上达到了 SOTA 性能。同时，预测出的方向信息可以确定实例的边界。[207]引入了概率 embeddings 进行点云的实例分割。该方法也继承了不确定估计并且提出了新的损失函数。

总体而言，不需要候选框的方法不需要耗费资源的区域生成步骤。然而，该方法的准确率较低因为该方法不检测物体的边界。

**4.3 Part Segmentation**

零件分割的主要困难来自于两方面。第一，有相同语义标签的部件有着较大的几何变化和不确定性；第二，该方法需要对噪声和采样具有鲁棒性。

[208] 提出了 VoxSegNet，在 3D 体素数据上来实现细粒度的零件分割。Spatial Dense Extraction(SDE) 模块用来在稀疏体素数据上提取大规模的具有判别性的 e 特征。学习到的特征被重新赋予权重，并且使用 Attention Feature Aggregation (AFA) 模块进行融合。[209] 将 FCN 与 surface-based CRF 组合，实现端到端的 3D 零件分割。他们首先从不同的视角产生图像来实现 optimal surface coverage，并将这些图片送入至 2D 网络产生置信 图。接着，使用 surface-based CRF 将置信图集成起来，用来对整个场景打标签。[210] 引入了 Synchronized Spectral CNN(SyncSpecCNN)，在不规则非同构形状图上实现卷积。

[211] 通过引入 Shape Fully Convolutional Networks(SFCN), 在 3D 网格上实现了形状分割，并且将三种低层次的几何特征作为输入。接着利用基于投票的多标签 graph cut 来修正分割结果。[212] 提出了弱监督的 CoSegNet 进行 3D 形状分割。该网络将一些未分割的 3D 点云形状作为输入，接着通过最小化 group consistency loss，产生形状零件的标签。与 CRF 类似，预训练的 part-refinement 网络用来修正并且去噪。[213] 提出了 Branched Auto-encoder network(BAE-NET) 用来无监督，one-shot 和弱监督 3D 形状分割。该方法将形状分割任务看做是特征学习问题并试图找到最简单的零件表示（通过最小化形状重建损失函数）。基于编码 - 解码的结构，该网络的每个分支都在学习特定零件形状的相容表示。学到的特征与点坐标一起送入解码器中，产生二进制的值（该值表示该点是否属于这一 part）。该方法有着良好的繁华性，并且可以处理大规模的 3D 形状几何。然而该方法对处值较为敏感，并且并未利用到形状的语义信息，妨碍了该方法在每次迭代中得到鲁棒、稳定的估计。

**4.4 Summary**

下表展示了已有方法在公开数据集上的结果，包括：S3DIS[176], Semantic3D[9], ScanNet[102] 和 SemanticKITTI[177].

![](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/07_15_07_15_v2-64ada45fef0cda311eb24920ab47014b_r.jpg)

接下来这些问题需要进一步的探索。

a. 基于点的网络是最常见的方法。然而，点的表示通常没有明确的邻域信息，大多数基于点的方法不得不试图使用好非自愿的邻域查找方法（KNN, ball query）。这自然地限制了这类方法的有效性，因为邻域查找方法需要很高的计算资源和内存。

b. 在点云分割中，从不平衡的数据中学习仍然是具有挑战性的问题。尽管许多方法 [42], [170], [182] 达到了不错的结果，但性能在较小类别的数据上仍然较差。

c. 大多数的方法 [5], [27], [52], [170], [171] 在较少点的点云上进行（4096）。实际上，从深度 sensor 上得到的点云是非常稠密的。因此需要寻求处理大规模点云的有效分割方法。

d. 一些工作 [145], [146], [167] 开始在动态点云中学习空间 - 时间的信息，空间 - 时间信息可以帮助提高 3D 目标检测，分割和补全是值得期待的。

**5 Conclusion**

本文章提出了当前针对 3D understanding 的一些 SOTA 方法，包括 3D 形状分类，3D 目标检测与跟踪以及 3D 场景和物体分割。

**6 Reference**

参考文献太多就不放了，见论文 “Deep Learning for 3D Point Clouds: A Survey” 里的参考文献。